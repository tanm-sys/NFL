#!/usr/bin/env python3
"""
Fine-tune NFL Trajectory Model from Best Checkpoint
====================================================

This script fine-tunes the best model checkpoint (epoch=04, ADE=0.5403)
with optimized hyperparameters to achieve better performance.

Improvements:
- 5x lower learning rate (0.0003)
- Increased weight decay (0.08)
- All 18 weeks of data
- Gradient accumulation for effective batch size 128
- Early stopping with patience=3
"""

import sys
sys.path.insert(0, 'src')
sys.path.insert(0, '.')

import json
import yaml
import argparse
import torch
# Enable Tensor Cores for maximum GPU performance
torch.set_float32_matmul_precision('medium')
import pytorch_lightning as pl
from pytorch_lightning.callbacks import (
    EarlyStopping,
    ModelCheckpoint,
    LearningRateMonitor,
    RichProgressBar,
    StochasticWeightAveraging,
)
from torch_geometric.loader import DataLoader as PyGDataLoader
from pathlib import Path
from rich.console import Console
from rich.panel import Panel

from src.train import NFLGraphPredictor
from src.data_loader import DataLoader, build_play_metadata, expand_play_tuples, GraphDataset

console = Console()

# Configuration
BEST_CHECKPOINT = "checkpoints/nfl_worldclass-epoch=04-val_ade=0.545.ckpt"
OUTPUT_DIR = Path("checkpoints_finetuned")
CACHE_DIR = Path("cache/finetune")

# Fine-tuning hyperparameters - RMSE OPTIMIZED (Verified: fits 4GB VRAM)
FINETUNE_CONFIG = {
    "lr": 0.0001,               # Standard LR
    "weight_decay": 0.05,       # Good regularization
    "max_epochs": 150,          # Long training
    "batch_size": 24,           # Good balance
    "accumulate_grad_batches": 8,  # Effective batch = 192
    "early_stopping_patience": 25,
    "weeks": list(range(1, 19)),  # ALL 18 weeks
    # Model architecture - VERIFIED 9.6M params, 2.1GB VRAM
    "hidden_dim": 256,          # Optimal for 4GB
    "num_gnn_layers": 6,        # 6 layers
    "heads": 8,                 # 8 heads
    "num_modes": 12,            # 12 modes
}


def create_dataloaders(weeks, batch_size, history_len=5, future_seq_len=10):
    """Create train/val dataloaders using all specified weeks."""
    loader = DataLoader(".")
    
    console.print(f"[cyan]Loading data for weeks: {weeks}[/cyan]")
    
    # Build play metadata
    play_meta = build_play_metadata(loader, weeks, history_len, future_seq_len)
    console.print(f"[green]Found {len(play_meta)} plays[/green]")
    
    # Split 80/20 by play
    split_idx = int(len(play_meta) * 0.8)
    train_meta = play_meta[:split_idx]
    val_meta = play_meta[split_idx:]
    
    # Expand to frame-level
    train_tuples = expand_play_tuples(train_meta)
    val_tuples = expand_play_tuples(val_meta)
    
    console.print(f"[green]Train samples: {len(train_tuples)}, Val samples: {len(val_tuples)}[/green]")
    
    # Create datasets with MAXIMUM caching for speed
    CACHE_DIR.mkdir(parents=True, exist_ok=True)
    
    train_dataset = GraphDataset(
        loader=loader,
        play_tuples=train_tuples,
        radius=20.0,
        future_seq_len=future_seq_len,
        history_len=history_len,
        cache_dir=CACHE_DIR / "train",
        persist_cache=True,
        in_memory_cache_size=100,  # LARGE cache for speed
    )
    
    val_dataset = GraphDataset(
        loader=loader,
        play_tuples=val_tuples,
        radius=20.0,
        future_seq_len=future_seq_len,
        history_len=history_len,
        cache_dir=CACHE_DIR / "val",
        persist_cache=True,
        in_memory_cache_size=50,  # LARGE cache for speed
    )
    
    train_loader = PyGDataLoader(
        train_dataset,
        batch_size=batch_size,
        shuffle=True,
        num_workers=0,  # Set to 0 to avoid PyG multiprocessing deadlock
        pin_memory=True,
    )
    
    val_loader = PyGDataLoader(
        val_dataset,
        batch_size=batch_size,
        shuffle=False,
        num_workers=0,
        pin_memory=True,
    )
    
    return train_loader, val_loader


def finetune():
    """Main fine-tuning function."""
    console.print(Panel.fit(
        "[bold green]ðŸš€ Fine-tuning NFL Model[/bold green]\n"
        f"Base checkpoint: {BEST_CHECKPOINT}",
        border_style="green"
    ))
    
    # Print config
    console.print("\n[cyan]Fine-tuning Configuration:[/cyan]")
    for k, v in FINETUNE_CONFIG.items():
        console.print(f"  {k}: {v}")
    
    # Setup output directory
    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)
    
    # Load base model (strict=False for backward compat with old checkpoints)
    console.print("\n[yellow]Loading base checkpoint...[/yellow]")
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    
    # Check if checkpoint exists, otherwise train from scratch
    if not Path(BEST_CHECKPOINT).exists():
        console.print(f"[yellow]âš ï¸ Checkpoint not found: {BEST_CHECKPOINT}[/yellow]")
        console.print("[cyan]Training MAXIMUM ACCURACY model from scratch...[/cyan]")
        model = NFLGraphPredictor(
            input_dim=9,
            hidden_dim=FINETUNE_CONFIG["hidden_dim"],
            lr=FINETUNE_CONFIG["lr"],
            probabilistic=True,
            num_modes=FINETUNE_CONFIG["num_modes"],
            use_social_nce=True,
            use_wta_loss=True,
            use_diversity_loss=True,
            use_endpoint_focal=True,
            weight_decay=FINETUNE_CONFIG["weight_decay"],
        )
    else:
        # Load with strict=False to allow loading old checkpoints without SOTA losses
        model = NFLGraphPredictor.load_from_checkpoint(
            BEST_CHECKPOINT,
            map_location=device,
            strict=False,  # Allow missing keys for new SOTA loss layers
            lr=FINETUNE_CONFIG["lr"],
            weight_decay=FINETUNE_CONFIG["weight_decay"],
        )
    
    console.print(f"[green]âœ“ Model loaded: {sum(p.numel() for p in model.parameters()):,} parameters[/green]")
    
    # Create dataloaders
    train_loader, val_loader = create_dataloaders(
        weeks=FINETUNE_CONFIG["weeks"],
        batch_size=FINETUNE_CONFIG["batch_size"],
    )
    
    # Setup callbacks
    callbacks = [
        EarlyStopping(
            monitor="val_ade",
            mode="min",
            patience=FINETUNE_CONFIG["early_stopping_patience"],
            verbose=True,
        ),
        ModelCheckpoint(
            dirpath=OUTPUT_DIR,
            filename="finetuned-{epoch:02d}-{val_ade:.4f}",
            monitor="val_ade",
            mode="min",
            save_top_k=3,
            verbose=True,
        ),
        LearningRateMonitor(logging_interval="epoch"),
        RichProgressBar(),
        StochasticWeightAveraging(
            swa_lrs=1e-5,
            swa_epoch_start=int(FINETUNE_CONFIG["max_epochs"] * 0.6),
            annealing_epochs=2,
        ),
    ]
    
    # Setup trainer with full GPU optimization
    trainer = pl.Trainer(
        max_epochs=FINETUNE_CONFIG["max_epochs"],
        accelerator="gpu" if torch.cuda.is_available() else "cpu",
        devices=1,
        callbacks=callbacks,
        accumulate_grad_batches=FINETUNE_CONFIG["accumulate_grad_batches"],
        gradient_clip_val=1.0,
        precision="16-mixed",  # Mixed precision for Tensor Cores
        log_every_n_steps=50,
        enable_progress_bar=True,
        benchmark=True if torch.cuda.is_available() else False,  # cuDNN autotuner
        deterministic=False,  # Non-deterministic for max speed
    )
    
    # NOTE: torch.compile disabled for fine-tuning - causes long warmup on first batch
    # Enable only for long training runs where compile overhead is amortized
    # if torch.cuda.is_available() and hasattr(torch, 'compile'):
    #     model.model = torch.compile(model.model, mode="default")
    
    console.print("\n[bold green]Starting fine-tuning...[/bold green]\n")
    
    # Train
    trainer.fit(model, train_loader, val_loader)
    
    # Get best result
    best_model_path = trainer.checkpoint_callback.best_model_path
    best_val_ade = trainer.checkpoint_callback.best_model_score
    
    console.print("\n" + "=" * 60)
    console.print(Panel.fit(
        f"[bold green]âœ… Fine-tuning Complete![/bold green]\n\n"
        f"Best checkpoint: {best_model_path}\n"
        f"Best val_ade: {best_val_ade:.4f}\n"
        f"Baseline was: 0.5403",
        border_style="green"
    ))
    
    # Calculate improvement
    if best_val_ade:
        improvement = ((0.5403 - best_val_ade) / 0.5403) * 100
        if improvement > 0:
            console.print(f"[bold green]ðŸ“ˆ Improvement: {improvement:.1f}%[/bold green]")
        else:
            console.print(f"[yellow]No improvement achieved (Î” = {improvement:.1f}%)[/yellow]")
    
    # Save config
    config_path = OUTPUT_DIR / "finetune_config.json"
    with open(config_path, "w") as f:
        json.dump(FINETUNE_CONFIG, f, indent=2)
    console.print(f"\nConfig saved to: {config_path}")
    
    return best_model_path, best_val_ade


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Fine-tune NFL model")
    parser.add_argument("--config", type=str, default=None, 
                        help="Path to YAML config file (e.g. configs/max_accuracy_rtx3050.yaml)")
    args = parser.parse_args()
    
    # Load config from YAML if provided
    if args.config:
        console.print(f"[cyan]Loading config from: {args.config}[/cyan]")
        with open(args.config, 'r') as f:
            yaml_config = yaml.safe_load(f)
        
        # Override FINETUNE_CONFIG with YAML values
        FINETUNE_CONFIG.update({
            "lr": yaml_config.get("learning_rate", FINETUNE_CONFIG["lr"]),
            "weight_decay": yaml_config.get("weight_decay", FINETUNE_CONFIG["weight_decay"]),
            "max_epochs": yaml_config.get("max_epochs", FINETUNE_CONFIG["max_epochs"]),
            "batch_size": yaml_config.get("batch_size", FINETUNE_CONFIG["batch_size"]),
            "accumulate_grad_batches": yaml_config.get("accumulate_grad_batches", FINETUNE_CONFIG["accumulate_grad_batches"]),
            "early_stopping_patience": yaml_config.get("early_stopping_patience", FINETUNE_CONFIG["early_stopping_patience"]),
            "weeks": yaml_config.get("weeks", FINETUNE_CONFIG["weeks"]),
            "hidden_dim": yaml_config.get("hidden_dim", FINETUNE_CONFIG["hidden_dim"]),
            "num_modes": yaml_config.get("num_modes", FINETUNE_CONFIG["num_modes"]),
        })
        console.print(f"[green]âœ“ Config loaded successfully![/green]")
    
    try:
        best_path, best_ade = finetune()
    except KeyboardInterrupt:
        console.print("\n[yellow]Training interrupted by user[/yellow]")
    except Exception as e:
        console.print(f"\n[red]Error: {e}[/red]")
        import traceback
        traceback.print_exc()
