{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# üèà NFL Analytics Engine - High Accuracy GPU Training\n",
                "\n",
                "This notebook trains the NFL trajectory prediction model with **maximum accuracy** while preventing overfitting/underfitting.\n",
                "\n",
                "## Setup Requirements\n",
                "1. **GPU Required**: Go to `Runtime ‚Üí Change runtime type ‚Üí GPU (T4 or A100 recommended)`\n",
                "2. **Training Data**: Upload your `input_2023_w*.csv` files to the `train/` directory\n",
                "3. **Estimated Time**: 2-4 hours on T4, 1-2 hours on A100"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1Ô∏è‚É£ GPU Verification & Setup"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Verify GPU availability\n",
                "import torch\n",
                "\n",
                "if torch.cuda.is_available():\n",
                "    gpu_name = torch.cuda.get_device_name(0)\n",
                "    gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
                "    print(f\"‚úÖ GPU Available: {gpu_name}\")\n",
                "    print(f\"   Memory: {gpu_memory:.1f} GB\")\n",
                "    \n",
                "    # Recommend batch size based on GPU\n",
                "    if gpu_memory >= 40:  # A100\n",
                "        recommended_batch = 128\n",
                "    elif gpu_memory >= 15:  # T4/V100\n",
                "        recommended_batch = 64\n",
                "    else:\n",
                "        recommended_batch = 32\n",
                "    print(f\"   Recommended batch size: {recommended_batch}\")\n",
                "else:\n",
                "    print(\"‚ùå No GPU detected!\")\n",
                "    print(\"   Go to Runtime ‚Üí Change runtime type ‚Üí GPU\")\n",
                "    raise SystemExit(\"GPU required for training\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Mount Google Drive (optional - for saving checkpoints)\n",
                "from google.colab import drive\n",
                "drive.mount('/content/drive')\n",
                "\n",
                "# Clone or upload project\n",
                "# Option 1: Clone from GitHub (replace with your repo)\n",
                "# !git clone https://github.com/YOUR_USERNAME/NFL.git\n",
                "# %cd NFL\n",
                "\n",
                "# Option 2: Upload from Drive\n",
                "# !cp -r /content/drive/MyDrive/NFL /content/NFL\n",
                "# %cd /content/NFL"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Run setup script\n",
                "!bash setup_colab.sh"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2Ô∏è‚É£ Verify Data"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "import glob\n",
                "\n",
                "# Check for training data\n",
                "data_files = glob.glob('train/input_2023_w*.csv')\n",
                "print(f\"Found {len(data_files)} week files:\")\n",
                "for f in sorted(data_files):\n",
                "    size_mb = os.path.getsize(f) / 1e6\n",
                "    print(f\"  - {os.path.basename(f)}: {size_mb:.1f} MB\")\n",
                "\n",
                "if len(data_files) == 0:\n",
                "    print(\"\\n‚ö†Ô∏è  No data files found!\")\n",
                "    print(\"   Upload input_2023_w*.csv files to the train/ directory\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3Ô∏è‚É£ Quick Sanity Check (2 min)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Run sanity check to verify everything works\n",
                "!python train_production.py --config configs/sanity.yaml"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4Ô∏è‚É£ High-Accuracy Training\n",
                "\n",
                "This configuration uses:\n",
                "- **Larger model**: 128 hidden dim, 6 GNN layers, 8 attention heads\n",
                "- **Anti-overfitting**: Dropout 0.15, early stopping (patience 15), data augmentation\n",
                "- **Anti-underfitting**: Warmup LR, 150 max epochs, multi-task loss\n",
                "- **Mixed precision**: FP16 for speed and larger batch sizes"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Start TensorBoard for monitoring\n",
                "%load_ext tensorboard\n",
                "%tensorboard --logdir logs/"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# üöÄ Start high-accuracy training\n",
                "!python train_production.py --config configs/high_accuracy.yaml"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5Ô∏è‚É£ Monitor Training Progress\n",
                "\n",
                "### What to look for:\n",
                "\n",
                "| Metric | Underfitting | Optimal | Overfitting |\n",
                "|--------|--------------|---------|-------------|\n",
                "| Train Loss | High & flat | Decreasing smoothly | Very low |\n",
                "| Val Loss | High | Follows train closely | Increases |\n",
                "| Train-Val Gap | Small but both high | Small and both low | Large gap |\n",
                "\n",
                "### Expected Results (after training):\n",
                "- **val_ade**: < 2.0 yards (Average Displacement Error)\n",
                "- **val_fde**: < 3.5 yards (Final Displacement Error)  \n",
                "- **miss_rate_2yd**: < 25% (Final position > 2 yards off)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# View training curves\n",
                "import matplotlib.pyplot as plt\n",
                "import pandas as pd\n",
                "from tensorboard.backend.event_processing import event_accumulator\n",
                "import glob\n",
                "\n",
                "# Find latest TensorBoard log\n",
                "log_dirs = glob.glob('logs/nfl_high_accuracy*/tensorboard/*')\n",
                "if log_dirs:\n",
                "    latest_log = sorted(log_dirs)[-1]\n",
                "    ea = event_accumulator.EventAccumulator(latest_log)\n",
                "    ea.Reload()\n",
                "    \n",
                "    # Plot training curves\n",
                "    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
                "    \n",
                "    metrics = [\n",
                "        ('train_loss', 'val_loss_traj', 'Loss'),\n",
                "        ('train_vel_loss', 'val_ade', 'ADE (yards)'),\n",
                "        ('val_fde', None, 'FDE (yards)'),\n",
                "        ('val_miss_rate_2yd', None, 'Miss Rate @ 2yd')\n",
                "    ]\n",
                "    \n",
                "    for ax, (train_key, val_key, title) in zip(axes.flat, metrics):\n",
                "        try:\n",
                "            if train_key and train_key in ea.scalars.Keys():\n",
                "                data = ea.scalars.Items(train_key)\n",
                "                ax.plot([x.step for x in data], [x.value for x in data], label='Train')\n",
                "            if val_key and val_key in ea.scalars.Keys():\n",
                "                data = ea.scalars.Items(val_key)\n",
                "                ax.plot([x.step for x in data], [x.value for x in data], label='Val')\n",
                "            ax.set_title(title)\n",
                "            ax.set_xlabel('Epoch')\n",
                "            ax.legend()\n",
                "            ax.grid(True, alpha=0.3)\n",
                "        except:\n",
                "            pass\n",
                "    \n",
                "    plt.tight_layout()\n",
                "    plt.show()\n",
                "else:\n",
                "    print(\"No training logs found yet. Run training first.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6Ô∏è‚É£ Save Best Model to Drive"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Copy best checkpoint to Google Drive\n",
                "import shutil\n",
                "\n",
                "best_ckpt = glob.glob('checkpoints/*best*.ckpt')\n",
                "if best_ckpt:\n",
                "    dest = '/content/drive/MyDrive/NFL_Models/'\n",
                "    os.makedirs(dest, exist_ok=True)\n",
                "    \n",
                "    for ckpt in best_ckpt:\n",
                "        shutil.copy(ckpt, dest)\n",
                "        print(f\"‚úÖ Saved: {ckpt} ‚Üí {dest}\")\n",
                "else:\n",
                "    print(\"No best checkpoint found yet.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 7Ô∏è‚É£ Troubleshooting\n",
                "\n",
                "### If training is **underfitting** (high loss, poor metrics):\n",
                "- Increase `hidden_dim` to 256\n",
                "- Add more layers (try `num_gnn_layers: 8`)\n",
                "- Decrease dropout to 0.1\n",
                "- Remove label smoothing\n",
                "\n",
                "### If training is **overfitting** (val loss increases while train decreases):\n",
                "- Increase dropout to 0.2\n",
                "- Increase weight_decay to 5e-4\n",
                "- Reduce hidden_dim to 64\n",
                "- Enable stronger augmentation\n",
                "\n",
                "### If training is **slow**:\n",
                "- Reduce batch_size if OOM errors\n",
                "- Reduce num_workers to 1\n",
                "- Use fewer weeks initially (weeks: [1, 2, 3])"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Custom training with adjusted parameters (if needed)\n",
                "# Uncomment and modify as needed:\n",
                "\n",
                "# !python train_production.py \\\n",
                "#     --config configs/high_accuracy.yaml \\\n",
                "#     --hidden-dim 256 \\\n",
                "#     --batch-size 32 \\\n",
                "#     --max-epochs 200 \\\n",
                "#     --learning-rate 0.0003"
            ]
        }
    ],
    "metadata": {
        "accelerator": "GPU",
        "colab": {
            "gpuType": "T4",
            "provenance": []
        },
        "kernelspec": {
            "display_name": "Python 3",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.10.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 0
}