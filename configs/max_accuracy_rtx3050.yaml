# ============================================================================
# üèÜ ULTIMATE SOTA PARAMETERS - COMPETITION WINNER GRADE
# ============================================================================
# Based on MTR v3 (2024 Waymo 1st Place) and Trajectron++ best practices
# Optimized for RTX 3050 4GB VRAM with maximum accuracy focus
#
# Target Metrics:
#   - minADE < 0.18 yards
#   - minFDE < 0.28 yards  
#   - ADE < 0.28 yards
#   - Miss Rate < 1%

experiment_name: "nfl_sota_winner"
description: "Competition-winning parameters based on MTR v3 and Trajectron++"

# ============================================================================
# DATA - FULL DATASET
# ============================================================================
data_dir: "."
weeks: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18]
radius: 25.0              # Larger radius for more context (MTR uses 16 neighbors)
future_seq_len: 10
history_len: 5

# ============================================================================
# MODEL ARCHITECTURE - MTR v3 INSPIRED
# ============================================================================
# MTR v3: 6 encoder + 6 decoder, hidden_dim=256
input_dim: 9
hidden_dim: 256           # Same as MTR v3
num_gnn_layers: 6         # 6 encoder layers (MTR v3)
num_decoder_layers: 6     # 6 decoder layers (MTR v3)  
heads: 8                  # Multi-head attention
dropout: 0.08             # Slightly lower for competition
droppath_rate: 0.15       # Stochastic depth (aggressive)

# Probabilistic Multi-Modal Prediction
probabilistic: true
num_modes: 12             # More modes = better minADE/minFDE
                          # MTR uses evolving anchors, we use GMM modes

# Advanced Encoders
use_scene_encoder: true
temporal_encoder_type: "hybrid"  # LSTM + Attention
use_goal_decoder: true           # Goal-conditioned (like MTR anchors)
use_hierarchical_decoder: true   # Coarse-to-fine

# ============================================================================
# TRAINING - COMPETITION GRADE
# ============================================================================
batch_size: 24             # Smaller for RTX 3050 with 12 modes
accumulate_grad_batches: 8  # Effective batch = 192 (large effective batch)
learning_rate: 0.0001      # MTR uses 1e-4 initial LR
max_epochs: 150            # Long training for convergence
min_epochs: 50

# Optimizer - AdamW (proven for transformers)
optimizer: "adamw"
weight_decay: 0.05         # Strong regularization (MTR-style)
betas: [0.9, 0.999]

# LR Scheduler - Cosine with long warmup
lr_scheduler: "cosine_warmup"
warmup_epochs: 10          # Longer warmup for stability
warmup_start_lr: 1.0e-06   # Start very low
min_lr: 1.0e-08            # Decay to near-zero

# ============================================================================
# LOSS WEIGHTS - OPTIMIZED FOR minADE/minFDE
# ============================================================================
# Primary losses
trajectory_weight: 1.0     # Main MSE loss
velocity_weight: 0.6       # Higher for smooth trajectories
acceleration_weight: 0.4   # Physical plausibility  
collision_weight: 0.20     # Collision avoidance
coverage_weight: 0.5       # Zone coverage

# ============================================================================
# SOTA CONTRASTIVE LOSSES - FULL POWER
# ============================================================================
# Social-NCE (social interaction learning)
use_social_nce: true
social_nce_weight: 0.20    # Higher weight for social learning
social_nce_temperature: 0.05  # Lower temp = sharper contrast

# Contrastive Learning
use_contrastive_loss: true
contrastive_weight: 0.15

# Winner-Takes-All (multi-modal training)
use_wta_loss: true
wta_k_best: 3              # Top-3 modes (more exploration)
wta_weight: 0.8            # Strong WTA signal

# Diversity (mode separation)
use_diversity_loss: true
diversity_weight: 0.06     # Higher for distinct modes
diversity_min_distance: 2.5  # Larger minimum separation

# Endpoint Focal (hard endpoint focus)
use_endpoint_focal: true
endpoint_focal_weight: 0.35  # Higher focus on endpoints (FDE)
endpoint_focal_gamma: 3.0    # Stronger focusing

# ============================================================================
# ADVANCED REGULARIZATION
# ============================================================================
gradient_clip_val: 0.3     # Tighter clipping for stability
use_augmentation: true     # Data augmentation
augmentation_rotate: true  # Rotation augmentation
augmentation_flip: true    # Horizontal flip
use_huber_loss: true       # Robust to outliers
huber_delta: 0.5           # Tighter delta
label_smoothing: 0.05      # Light smoothing
use_ema: true              # Exponential Moving Average
ema_decay: 0.999           # EMA decay rate

# ============================================================================
# HARDWARE - RTX 3050 4GB OPTIMIZED
# ============================================================================
precision: "16-mixed"
accelerator: "gpu"
devices: 1
benchmark: true
deterministic: false
num_workers: 0             # Pre-cached graphs

# Memory optimization
in_memory_cache_size: 150  # RAM cache
persist_cache: true
cache_dir: "cache/finetune/train"

# ============================================================================
# CALLBACKS - AGGRESSIVE MONITORING
# ============================================================================
early_stopping_patience: 25   # Very patient
early_stopping_min_delta: 0.00005  # Tiny improvement detection
monitor_metric: "val_minADE"  # Monitor best multi-modal metric
save_top_k: 10                # Keep more checkpoints
save_last: true

# SWA - Stochastic Weight Averaging
swa_enabled: true
swa_epoch_start: 0.70         # Start at 70%
swa_lrs: 1.0e-06
swa_annealing_epochs: 15

# Model Ensemble (inference only)
ensemble_enabled: true
ensemble_top_k: 3             # Average top-3 checkpoints

# ============================================================================
# COMPETITION TARGETS (AGGRESSIVE)
# ============================================================================
target_val_ade: 0.28
target_val_minADE: 0.18
target_val_fde: 0.42
target_val_minFDE: 0.28
target_val_miss_rate: 0.01

# ============================================================================
# NOTES
# ============================================================================
# Based on:
# - MTR v3 (Waymo 2024 1st place): 6 encoder/decoder, hidden=256, LR=1e-4
# - Trajectron++: Probabilistic multi-modal, social modeling
# - Best practices: WTA for multi-modal, SWA for generalization
#
# For RTX 3050 4GB:
# - batch_size=24 with 12 modes fits in 4GB
# - Pre-cache graphs to eliminate CPU bottleneck
# - accumulate_grad=8 for effective batch of 192
