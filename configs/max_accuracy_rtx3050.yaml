# ============================================================================
# üèÜ MAXIMUM PARAMETERS CONFIG - LARGEST MODEL FOR RTX 3050 4GB
# ============================================================================
# Pushing model capacity to the absolute limit of 4GB VRAM
# Every parameter increased for maximum representational power
#
# Target: ~12-15M parameters (3x larger than default)

experiment_name: "nfl_max_parameters"
description: "Maximum model size for RTX 3050 4GB - competition grade"

# ============================================================================
# DATA - FULL DATASET
# ============================================================================
data_dir: "."
weeks: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18]
radius: 30.0              # Maximum radius for full context
future_seq_len: 10
history_len: 5

# ============================================================================
# MODEL ARCHITECTURE - MAXIMUM SIZE
# ============================================================================
input_dim: 9
hidden_dim: 384           # INCREASED from 256 (1.5x encoder capacity)
num_gnn_layers: 8         # INCREASED - deep network
num_decoder_layers: 6     # 6 decoder layers
heads: 12                 # INCREASED - more attention heads
dropout: 0.08
droppath_rate: 0.18       # Higher for larger model

# Probabilistic Multi-Modal - MAXIMUM
probabilistic: true
num_modes: 16             # MAXIMUM modes for best minADE/minFDE

# Advanced Components - ALL ENABLED
use_scene_encoder: true
scene_encoder_inducing: 12  # More inducing points
temporal_encoder_type: "hybrid"
temporal_hidden: 128        # Larger temporal encoder
use_goal_decoder: true
use_hierarchical_decoder: true
use_social_pooling: true

# Embedding Dimensions - INCREASED
role_emb_dim: 128         # Larger embeddings
side_emb_dim: 64
formation_emb_dim: 128
alignment_emb_dim: 96

# ============================================================================
# TRAINING - CAREFUL WITH LARGE MODEL
# ============================================================================
batch_size: 16             # Smaller batch for large model
accumulate_grad_batches: 12  # Effective batch = 192
learning_rate: 0.00008     # Lower LR for large model stability
max_epochs: 200            # Very long training
min_epochs: 80

# Optimizer
optimizer: "adamw"
weight_decay: 0.08         # Stronger regularization for large model
betas: [0.9, 0.98]         # Slightly different beta2

# LR Scheduler
lr_scheduler: "cosine_warmup"
warmup_epochs: 15          # Longer warmup for large model
warmup_start_lr: 1.0e-07
min_lr: 1.0e-09

# ============================================================================
# LOSS WEIGHTS - MAXIMUM POWER
# ============================================================================
trajectory_weight: 1.0
velocity_weight: 0.7       # Higher for smooth motion
acceleration_weight: 0.5   # Physical plausibility
collision_weight: 0.25     # Strong collision avoidance
coverage_weight: 0.6

# ============================================================================
# SOTA LOSSES - ALL AT MAXIMUM
# ============================================================================
use_social_nce: true
social_nce_weight: 0.25    # Maximum social learning
social_nce_temperature: 0.04

use_contrastive_loss: true
contrastive_weight: 0.20

use_wta_loss: true
wta_k_best: 4              # Top-4 modes for 16-mode model
wta_weight: 1.0

use_diversity_loss: true
diversity_weight: 0.08     # Strong diversity for 16 modes
diversity_min_distance: 3.0  # Large separation

use_endpoint_focal: true
endpoint_focal_weight: 0.40  # Maximum endpoint focus
endpoint_focal_gamma: 3.5

# ============================================================================
# ADVANCED REGULARIZATION
# ============================================================================
gradient_clip_val: 0.25    # Tight clipping for large model
use_augmentation: true
augmentation_rotate: true
augmentation_flip: true
augmentation_scale: true   # Scale augmentation
use_huber_loss: true
huber_delta: 0.4           # Tighter delta
label_smoothing: 0.03
use_ema: true
ema_decay: 0.9995          # Slower EMA for large model

# ============================================================================
# HARDWARE - MAXIMUM UTILIZATION
# ============================================================================
precision: "16-mixed"
accelerator: "gpu"
devices: 1
benchmark: true
deterministic: false
num_workers: 0

# Memory - minimal caching to fit model
in_memory_cache_size: 100
persist_cache: true
cache_dir: "cache/finetune/train"

# ============================================================================
# CALLBACKS
# ============================================================================
early_stopping_patience: 30   # Very patient for large model
early_stopping_min_delta: 0.00002
monitor_metric: "val_minADE"
save_top_k: 15
save_last: true

# SWA
swa_enabled: true
swa_epoch_start: 0.65
swa_lrs: 5.0e-07
swa_annealing_epochs: 20

# ============================================================================
# TARGET METRICS (WORLD-CLASS)
# ============================================================================
target_val_ade: 0.22
target_val_minADE: 0.14
target_val_fde: 0.35
target_val_minFDE: 0.22
target_val_miss_rate: 0.005

# ============================================================================
# ESTIMATED MODEL SIZE
# ============================================================================
# hidden_dim=384, 8 layers, 12 heads, 16 modes
# Encoder: ~8M params
# Decoder: ~3M params  
# Losses: ~15K params
# TOTAL: ~11-12M parameters
#
# VRAM Usage (estimated):
# - Model: ~800 MB
# - Batch 16: ~1.2 GB
# - Gradients: ~1.5 GB
# - Optimizer: ~500 MB
# - TOTAL: ~4.0 GB (fits in RTX 3050!)
